"""
æ­£å¼å¯¹æŠ—è®­ç»ƒè„šæœ¬

æ•°æ®é›†ï¼š
- query/qa_with_paths_cleaned.json (å®Œæ•´æ•°æ®é›†)
- graph/knowledge_graph.pkl (çŸ¥è¯†å›¾è°±)
- graph/entity_names.json (å®ä½“åç§°)

è®­ç»ƒç­–ç•¥ï¼š
- åªä½¿ç”¨ "xhop_train" æ•°æ®
- æ¯ä¸ªepochéšæœºé€‰æ‹©ä¸åŒhopè¿›è¡Œè®­ç»ƒ
- ä½¿ç”¨ä¿®å¤ç‰ˆGANRLTrainerFixedå¤„ç†"åŒæ–¹éƒ½çŠ¯é”™"æƒ…å†µ
"""

import sys
import torch
import pickle
import json
import random
import time
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Set

# æ·»åŠ modelsè·¯å¾„
sys.path.append(str(Path(__file__).parent / "models" / "path_discover"))
sys.path.append(str(Path(__file__).parent / "models" / "path_ranker"))

from differentiable_path_generator_truly_fixed import DifferentiablePathGeneratorTrulyFixed
from enhanced_path_ranker import EnhancedPathRankerDiscriminator
from gan_rl_trainer_fixed import GANRLTrainerFixed

class ProductionTrainingPipeline:
    """ç”Ÿäº§ç¯å¢ƒè®­ç»ƒæµæ°´çº¿"""
    
    def __init__(self):
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"ğŸ–¥ï¸  Using device: {self.device}")
        
        self.generator = None
        self.discriminator = None
        self.trainer = None
        
        # æ•°æ®
        self.knowledge_graph = None
        self.entity_names = None
        self.train_data = {}  # æŒ‰hopç±»å‹ç»„ç»‡çš„è®­ç»ƒæ•°æ®
        
        # è®­ç»ƒç»Ÿè®¡
        self.training_history = {
            'epochs': [],
            'hop_distribution': [],
            'adversarial_stats': []
        }
    
    def load_complete_dataset(self):
        """åŠ è½½å®Œæ•´æ•°æ®é›†"""
        print("ğŸ“Š Loading complete dataset...")
        
        # 1. åŠ è½½QAæ•°æ®
        qa_file = "query/qa_with_paths_cleaned.json"
        if not Path(qa_file).exists():
            print(f"âŒ QA dataset not found: {qa_file}")
            return False
        
        print(f"ğŸ“– Loading QA dataset from {qa_file}")
        all_data = self._load_multiline_json(qa_file)
        
        if not all_data:
            print(f"âŒ Failed to load QA dataset")
            return False
        
        print(f"âœ… Loaded {len(all_data)} total QA samples")
        
        # 2. ç­›é€‰è®­ç»ƒæ•°æ®å¹¶æŒ‰hopåˆ†ç»„
        self.train_data = {
            '1hop': [],
            '2hop': [], 
            '3hop': []
        }
        
        for item in all_data:
            type_field = item.get('type', '')
            if 'train' in type_field:
                if '1hop' in type_field:
                    self.train_data['1hop'].append(item)
                elif '2hop' in type_field:
                    self.train_data['2hop'].append(item)
                elif '3hop' in type_field:
                    self.train_data['3hop'].append(item)
        
        print(f"ğŸ“ˆ Training data distribution:")
        for hop_type, data in self.train_data.items():
            print(f"   {hop_type}: {len(data)} samples")
        
        total_train = sum(len(data) for data in self.train_data.values())
        print(f"   Total training samples: {total_train}")
        
        if total_train == 0:
            print(f"âŒ No training data found!")
            return False
        
        # 3. åŠ è½½çŸ¥è¯†å›¾è°±
        kg_file = "graph/knowledge_graph.pkl"
        if not Path(kg_file).exists():
            print(f"âŒ Knowledge graph not found: {kg_file}")
            return False
        
        print(f"ğŸ•¸ï¸  Loading knowledge graph from {kg_file}")
        with open(kg_file, 'rb') as f:
            self.knowledge_graph = pickle.load(f)
        
        print(f"âœ… Knowledge graph loaded: {self.knowledge_graph.number_of_nodes()} nodes, {self.knowledge_graph.number_of_edges()} edges")
        
        # 4. åŠ è½½å®ä½“åç§°ï¼ˆå¯é€‰ï¼‰
        entity_file = "graph/entity_names.json"
        if Path(entity_file).exists():
            print(f"ğŸ“ Loading entity names from {entity_file}")
            with open(entity_file, 'r', encoding='utf-8') as f:
                self.entity_names = json.load(f)
            print(f"âœ… Loaded {len(self.entity_names)} entity names")
        
        return True
    
    def _load_multiline_json(self, file_path: str) -> List[Dict]:
        """åŠ è½½å¤šè¡ŒJSONæ ¼å¼çš„æ–‡ä»¶"""
        data = []
        current_obj = ""
        brace_count = 0
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if not line:
                        continue
                    
                    current_obj += line + "\\n"
                    
                    # è®¡ç®—å¤§æ‹¬å·
                    brace_count += line.count('{') - line.count('}')
                    
                    # å½“å¤§æ‹¬å·å¹³è¡¡æ—¶ï¼Œè¯´æ˜ä¸€ä¸ªJSONå¯¹è±¡ç»“æŸ
                    if brace_count == 0 and current_obj.strip():
                        try:
                            obj = json.loads(current_obj.strip())
                            data.append(obj)
                            current_obj = ""
                        except json.JSONDecodeError as e:
                            print(f"JSON parse error at line {line_num}: {e}")
                            print(f"Object: {current_obj[:100]}...")
                            current_obj = ""
                            
        except Exception as e:
            print(f"File read error: {e}")
            return []
        
        return data
    
    def initialize_models(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        print("\\nğŸ¤– Initializing models...")
        
        # æ£€æŸ¥å¿…è¦æ–‡ä»¶
        if not Path("embeddings/entity_embeddings.pt").exists():
            print("âŒ Entity embeddings not found. Run: python create_embeddings.py")
            return False
        
        try:
            # 1. åˆå§‹åŒ–ç”Ÿæˆå™¨ (Discoverer)
            print("âš¡ Initializing Discoverer (Generator)...")
            self.generator = DifferentiablePathGeneratorTrulyFixed(
                entity_embedding_path="embeddings/entity_embeddings.pt",
                max_path_length=6,
                beam_width=5
            )
            self.generator.knowledge_graph = self.knowledge_graph  # è®¾ç½®KG
            print("âœ… Discoverer initialized")
            
            # 2. åˆå§‹åŒ–åˆ¤åˆ«å™¨ (Ranker)
            print("ğŸ§  Initializing Ranker (Discriminator)...")
            self.discriminator = EnhancedPathRankerDiscriminator(
                freeze_sbert=True,
                disable_pattern_memory=True
            )
            print("âœ… Ranker initialized")
            
            # 3. åˆå§‹åŒ–ä¿®å¤ç‰ˆè®­ç»ƒå™¨
            print("ğŸ”§ Initializing Fixed GAN-RL Trainer...")
            self.trainer = GANRLTrainerFixed(
                self.generator, 
                self.discriminator, 
                device=self.device
            )
            print("âœ… Fixed GAN-RL Trainer initialized")
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            self.generator.to(self.device)
            self.discriminator.to(self.device)
            
            print(f"ğŸ¯ All models ready on {self.device}")
            return True
            
        except Exception as e:
            print(f"âŒ Model initialization failed: {e}")
            return False
    
    def create_epoch_training_plan(self, epoch: int) -> Dict[str, List[Dict]]:
        """
        åˆ›å»ºæœ¬è½®è®­ç»ƒè®¡åˆ’ï¼šéšæœºé€‰æ‹©ä¸åŒhopç±»å‹è¿›è¡Œè®­ç»ƒ
        
        ç­–ç•¥ï¼š
        - æ¯ä¸ªepochéšæœºé€‰æ‹©hopç±»å‹å’Œæ ·æœ¬
        - ç¡®ä¿å„hopç±»å‹éƒ½æœ‰è®­ç»ƒæœºä¼š
        - åŠ¨æ€å¹³è¡¡ä¸åŒå¤æ‚åº¦çš„è®­ç»ƒ
        """
        print(f"\\nğŸ“‹ Planning training for Epoch {epoch + 1}...")
        
        # éšæœºé€‰æ‹©hopç±»å‹é¡ºåº
        hop_types = ['1hop', '2hop', '3hop']
        random.shuffle(hop_types)
        
        epoch_plan = {}
        
        for hop_type in hop_types:
            available_data = self.train_data[hop_type]
            if not available_data:
                continue
            
            # æ ¹æ®æ•°æ®é‡åŠ¨æ€é€‰æ‹©æ ·æœ¬æ•°
            if len(available_data) > 100:
                # å¤§æ•°æ®é‡ï¼šéšæœºé‡‡æ ·
                sample_count = min(50, len(available_data) // 2)
                selected_samples = random.sample(available_data, sample_count)
            else:
                # å°æ•°æ®é‡ï¼šä½¿ç”¨å…¨éƒ¨
                selected_samples = available_data.copy()
                random.shuffle(selected_samples)
            
            epoch_plan[hop_type] = selected_samples
            print(f"   {hop_type}: {len(selected_samples)} samples")
        
        total_samples = sum(len(samples) for samples in epoch_plan.values())
        print(f"   ğŸ“Š Total epoch samples: {total_samples}")
        
        # è®°å½•hopåˆ†å¸ƒ
        hop_distribution = {hop: len(samples) for hop, samples in epoch_plan.items()}
        self.training_history['hop_distribution'].append(hop_distribution)
        
        return epoch_plan
    
    def train_with_hop_curriculum(self, epochs: int = 10):
        """
        æŒ‰hopè¯¾ç¨‹è¿›è¡Œå¯¹æŠ—è®­ç»ƒ
        
        æ¯ä¸ªepochï¼š
        1. éšæœºé€‰æ‹©hopç±»å‹å’Œæ ·æœ¬
        2. ä½¿ç”¨ä¿®å¤ç‰ˆè®­ç»ƒå™¨å¤„ç†"åŒæ–¹éƒ½çŠ¯é”™"
        3. è®°å½•å¯¹æŠ—å­¦ä¹ ç»Ÿè®¡ä¿¡æ¯
        """
        print(f"\\nğŸš€ Starting Production Adversarial Training ({epochs} epochs)")
        print("="*80)
        
        for epoch in range(epochs):
            epoch_start_time = time.time()
            print(f"\\nğŸ¯ EPOCH {epoch + 1}/{epochs}")
            print("-" * 60)
            
            try:
                # 1. åˆ›å»ºæœ¬è½®è®­ç»ƒè®¡åˆ’
                epoch_plan = self.create_epoch_training_plan(epoch)
                
                if not epoch_plan:
                    print("âš ï¸  No training data for this epoch, skipping...")
                    continue
                
                # 2. æŒ‰hopç±»å‹ä¾æ¬¡è®­ç»ƒ
                epoch_stats = {
                    'epoch': epoch + 1,
                    'hop_results': {},
                    'overall_discriminator_loss': 0.0,
                    'overall_generator_reward': 0.0,
                    'timestamp': datetime.now().isoformat()
                }
                
                total_disc_loss = 0.0
                total_gen_reward = 0.0
                hop_count = 0
                
                for hop_type, samples in epoch_plan.items():
                    if not samples:
                        continue
                        
                    print(f"\\nğŸ” Training on {hop_type} data ({len(samples)} samples)...")
                    
                    # ä½¿ç”¨ä¿®å¤ç‰ˆè®­ç»ƒå™¨
                    hop_result = self.trainer.train_epoch_with_adversarial_correction(
                        samples, current_epoch=epoch
                    )
                    
                    epoch_stats['hop_results'][hop_type] = hop_result
                    total_disc_loss += hop_result.get('discriminator_loss', 0.0)
                    total_gen_reward += hop_result.get('generator_reward', 0.0)
                    hop_count += 1
                    
                    print(f"âœ… {hop_type} training completed:")
                    print(f"   Discriminator Loss: {hop_result.get('discriminator_loss', 0):.4f}")
                    print(f"   Generator Reward: {hop_result.get('generator_reward', 0):.4f}")
                    print(f"   Discriminator Fooled: {hop_result.get('discriminator_fooled', 0)} cases")
                    print(f"   GT Corrections: {hop_result.get('ground_truth_corrections', 0)} cases")
                
                # 3. æ±‡æ€»epochç»Ÿè®¡
                epoch_stats['overall_discriminator_loss'] = total_disc_loss / max(1, hop_count)
                epoch_stats['overall_generator_reward'] = total_gen_reward / max(1, hop_count)
                
                # 4. è·å–å¯¹æŠ—å­¦ä¹ ç»Ÿè®¡
                adversarial_stats = self.trainer.get_adversarial_stats()
                epoch_stats['adversarial_stats'] = adversarial_stats
                self.training_history['adversarial_stats'].append(adversarial_stats)
                
                epoch_time = time.time() - epoch_start_time
                epoch_stats['training_time'] = epoch_time
                
                print(f"\\nğŸ“Š EPOCH {epoch + 1} SUMMARY:")
                print(f"   Overall Discriminator Loss: {epoch_stats['overall_discriminator_loss']:.4f}")
                print(f"   Overall Generator Reward: {epoch_stats['overall_generator_reward']:.4f}")
                print(f"   ğŸ­ Discriminator Fooled Total: {adversarial_stats['discriminator_fooled_total']}")
                print(f"   ğŸ¯ GT Corrections Applied: {adversarial_stats['discriminator_corrected_total']}")
                print(f"   ğŸ“ˆ Correction Effectiveness: {adversarial_stats['correction_effectiveness']:.1%}")
                print(f"   â±ï¸  Training Time: {epoch_time:.2f}s")
                
                self.training_history['epochs'].append(epoch_stats)
                
                # 5. å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
                if (epoch + 1) % 5 == 0:
                    self.save_training_checkpoint(epoch + 1)
                
            except Exception as e:
                print(f"âŒ Epoch {epoch + 1} failed: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        print(f"\\nğŸŠ Production Training Completed!")
        self.save_final_models()
        self.generate_training_report()
    
    def save_training_checkpoint(self, epoch: int):
        """ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹"""
        try:
            checkpoint_dir = Path("checkpoints")
            checkpoint_dir.mkdir(exist_ok=True)
            
            checkpoint_path = checkpoint_dir / f"production_training_epoch_{epoch}.pt"
            self.trainer.save_checkpoint(str(checkpoint_path), epoch)
            
            # ä¿å­˜è®­ç»ƒå†å²
            history_path = checkpoint_dir / f"training_history_epoch_{epoch}.json"
            with open(history_path, 'w', encoding='utf-8') as f:
                json.dump(self.training_history, f, indent=2, ensure_ascii=False)
            
            print(f"ğŸ’¾ Checkpoint saved: {checkpoint_path}")
            
        except Exception as e:
            print(f"âŒ Checkpoint save failed: {e}")
    
    def save_final_models(self):
        """ä¿å­˜æœ€ç»ˆç”Ÿäº§æ¨¡å‹"""
        try:
            from advanced_inference_system import save_production_models
            
            # ä¿å­˜ç”Ÿäº§ç¯å¢ƒæ¨¡å‹
            production_dir = save_production_models(
                self.trainer, 
                "checkpoints/production_models_final"
            )
            
            # ä¿å­˜å®Œæ•´è®­ç»ƒå†å²
            history_file = Path(production_dir) / "complete_training_history.json"
            with open(history_file, 'w', encoding='utf-8') as f:
                json.dump(self.training_history, f, indent=2, ensure_ascii=False)
            
            print(f"ğŸ‰ Final models saved to: {production_dir}")
            
        except Exception as e:
            print(f"âŒ Final model save failed: {e}")
    
    def generate_training_report(self):
        """ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š"""
        print("\\nğŸ“Š FINAL TRAINING REPORT")
        print("="*80)
        
        if not self.training_history['epochs']:
            print("âŒ No training history available")
            return
        
        # æ•´ä½“ç»Ÿè®¡
        total_epochs = len(self.training_history['epochs'])
        final_stats = self.training_history['adversarial_stats'][-1] if self.training_history['adversarial_stats'] else {}
        
        print(f"ğŸ¯ Training Overview:")
        print(f"   Total Epochs: {total_epochs}")
        print(f"   Total Training Data:")
        for hop_type, data in self.train_data.items():
            print(f"     {hop_type}: {len(data)} samples")
        
        # å¯¹æŠ—å­¦ä¹ æ•ˆæœ
        print(f"\\nğŸ­ Adversarial Learning Results:")
        print(f"   Discriminator Fooled Cases: {final_stats.get('discriminator_fooled_total', 0)}")
        print(f"   Ground Truth Corrections: {final_stats.get('discriminator_corrected_total', 0)}")
        print(f"   Generator Misled Cases: {final_stats.get('generator_misled_total', 0)}")
        print(f"   Correction Effectiveness: {final_stats.get('correction_effectiveness', 0):.1%}")
        
        # hopåˆ†å¸ƒç»Ÿè®¡
        if self.training_history['hop_distribution']:
            print(f"\\nğŸ“ˆ Hop Distribution Across Epochs:")
            hop_totals = {'1hop': 0, '2hop': 0, '3hop': 0}
            for epoch_dist in self.training_history['hop_distribution']:
                for hop_type, count in epoch_dist.items():
                    hop_totals[hop_type] += count
            
            for hop_type, total in hop_totals.items():
                print(f"   {hop_type}: {total} total samples trained")
        
        # æ€§èƒ½è¶‹åŠ¿
        if len(self.training_history['epochs']) > 1:
            first_epoch = self.training_history['epochs'][0]
            last_epoch = self.training_history['epochs'][-1]
            
            disc_improvement = last_epoch['overall_discriminator_loss'] - first_epoch['overall_discriminator_loss']
            gen_improvement = last_epoch['overall_generator_reward'] - first_epoch['overall_generator_reward']
            
            print(f"\\nğŸ“Š Performance Trends:")
            print(f"   Discriminator Loss Change: {disc_improvement:+.4f}")
            print(f"   Generator Reward Change: {gen_improvement:+.4f}")
        
        print(f"\\nğŸš€ Models ready for production use!")
        print(f"   Use: python demo_advanced_inference.py")

def main():
    """ä¸»è®­ç»ƒç¨‹åº"""
    print("ğŸ¯ Production Adversarial Training Pipeline")
    print("="*80)
    
    # åˆå§‹åŒ–è®­ç»ƒæµæ°´çº¿
    pipeline = ProductionTrainingPipeline()
    
    # 1. åŠ è½½å®Œæ•´æ•°æ®é›†
    if not pipeline.load_complete_dataset():
        print("âŒ Dataset loading failed. Exiting.")
        return
    
    # 2. åˆå§‹åŒ–æ¨¡å‹
    if not pipeline.initialize_models():
        print("âŒ Model initialization failed. Exiting.")
        return
    
    # 3. å¼€å§‹å¯¹æŠ—è®­ç»ƒ
    print("\\nğŸš€ Starting adversarial training...")
    print("ğŸ’¡ This will use the FIXED GAN-RL trainer that properly handles:")
    print("   âœ… Discriminator fooling detection")
    print("   âœ… Ground Truth correction mechanism") 
    print("   âœ… Intelligent reward shaping")
    print("   âœ… True adversarial learning spiral")
    
    # è®­ç»ƒå‚æ•°
    epochs = 15  # æ›´å¤šepochä»¥å……åˆ†å¯¹æŠ—è®­ç»ƒ
    
    pipeline.train_with_hop_curriculum(epochs=epochs)
    
    print("\\nğŸŠ Production training completed successfully!")
    print("\\nNext steps:")
    print("1. ğŸ§ª Test the trained models: python demo_advanced_inference.py")
    print("2. ğŸ” Analyze training history in checkpoints/")
    print("3. ğŸš€ Deploy the production models")

if __name__ == "__main__":
    main()