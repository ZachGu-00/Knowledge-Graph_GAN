"""
æµ‹è¯•A*è·¯å¾„ç”Ÿæˆå™¨ + å¯¹æŠ—å­¦ä¹ è®­ç»ƒ

åŠŸèƒ½ï¼š
1. A*ç”Ÿæˆå™¨åŸºæœ¬åŠŸèƒ½æµ‹è¯•
2. åŠ è½½è®­ç»ƒå¥½çš„åˆ¤åˆ«å™¨
3. è¿è¡ŒGAN-RLå¯¹æŠ—å­¦ä¹ è®­ç»ƒ
4. ä½¿ç”¨10ä¸ªç®€å•1hopæ ·æœ¬
5. è¾“å‡ºè¯¦ç»†çš„è®­ç»ƒç»“æœåˆ°JSON
"""

import sys
import torch
import pickle
import json
import time
from datetime import datetime
from pathlib import Path

# æ·»åŠ modelsè·¯å¾„
sys.path.append(str(Path(__file__).parent / "models" / "path_discover"))
sys.path.append(str(Path(__file__).parent / "models" / "path_ranker"))

from astar_path_generator import AStarPathGenerator
from differentiable_path_generator_truly_fixed import DifferentiablePathGeneratorTrulyFixed
from gan_rl_trainer import GANRLTrainer
from enhanced_path_ranker import EnhancedPathRankerDiscriminator

def test_astar_generator():
    """æµ‹è¯•A*ç”Ÿæˆå™¨çš„åŸºæœ¬åŠŸèƒ½"""
    
    print("A* Path Generator Test")
    print("="*50)
    
    # 1. æ£€æŸ¥å¿…éœ€æ–‡ä»¶
    required_files = {
        "embeddings/entity_embeddings.pt": "Entity embeddings",
        "query/qa_with_paths_cleaned.json": "QA dataset",
        "graph/knowledge_graph.pkl": "Knowledge graph"
    }
    
    for file_path, description in required_files.items():
        if not Path(file_path).exists():
            print(f"Error: {description} not found at {file_path}")
            return
        else:
            print(f"[OK] {description} found")
    
    print(f"\n{'='*50}")
    print("INITIALIZING A* GENERATOR")
    print("="*50)
    
    # 2. åˆå§‹åŒ–A*ç”Ÿæˆå™¨
    try:
        generator = AStarPathGenerator(
            entity_embedding_path="embeddings/entity_embeddings.pt",
            max_path_length=6,
            beam_width=5
        )
        print("[OK] A* Generator initialized successfully")
        
        # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in generator.parameters())
        trainable_params = sum(p.numel() for p in generator.parameters() if p.requires_grad)
        print(f"[INFO] Total parameters: {total_params:,}")
        print(f"[INFO] Trainable parameters: {trainable_params:,}")
        print(f"[INFO] SBERT dimension: {generator.sbert_dim}")
        
    except Exception as e:
        print(f"[ERROR] Failed to initialize generator: {e}")
        return
    
    # 3. åŠ è½½çŸ¥è¯†å›¾è°±
    print(f"\n{'='*50}")
    print("LOADING KNOWLEDGE GRAPH")
    print("="*50)
    
    try:
        with open("graph/knowledge_graph.pkl", 'rb') as f:
            knowledge_graph = pickle.load(f)
        
        print(f"[OK] Knowledge graph loaded: {len(knowledge_graph.nodes)} nodes, {len(knowledge_graph.edges)} edges")
        
        # åˆ›å»ºç®€å•çš„å…³ç³»è¯è¡¨ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        relations = set()
        for _, _, edge_data in knowledge_graph.edges(data=True):
            relation = edge_data.get('relation', 'related_to')
            relations.add(relation)
        
        relation_to_id = {rel: idx for idx, rel in enumerate(relations)}
        print(f"[OK] Found {len(relation_to_id)} unique relations")
        
        # è®¾ç½®çŸ¥è¯†å›¾è°±
        generator.set_knowledge_graph(knowledge_graph, relation_to_id)
        print("[OK] Knowledge graph set in generator")
        
    except Exception as e:
        print(f"[ERROR] Failed to load knowledge graph: {e}")
        return
    
    # 4. åŠ è½½æµ‹è¯•æ ·æœ¬
    print(f"\n{'='*50}")
    print("LOADING TEST SAMPLES")
    print("="*50)
    
    try:
        with open("query/qa_with_paths_cleaned.json", 'r', encoding='utf-8') as f:
            content = f.read()
        
        query_blocks = content.strip().split('\\n\\n')[:3]  # å–å‰3ä¸ªæ ·æœ¬æµ‹è¯•
        test_samples = []
        
        for block in query_blocks:
            if block.strip():
                try:
                    sample = json.loads(block)
                    test_samples.append(sample)
                except:
                    continue
        
        print(f"[OK] Loaded {len(test_samples)} test samples")
        
    except Exception as e:
        print(f"[ERROR] Failed to load test samples: {e}")
        return
    
    # 5. æµ‹è¯•è·¯å¾„ç”Ÿæˆ
    print(f"\n{'='*50}")
    print("TESTING PATH GENERATION")
    print("="*50)
    
    for i, sample in enumerate(test_samples):
        question = sample['question']
        question_entity = sample['question_entity']
        answer_entities = set(sample['answer_entities'])
        query_type = sample.get('type', '')
        
        print(f"\n--- Test Sample {i+1} ---")
        print(f"Start entity: {question_entity}")
        print(f"Target entities: {list(answer_entities)[:3]}...")  # åªæ˜¾ç¤ºå‰3ä¸ª
        print(f"Query type: {query_type}")
        
        # æ£€æŸ¥èµ·å§‹å®ä½“æ˜¯å¦åœ¨å›¾ä¸­
        if question_entity not in knowledge_graph:
            print(f"[SKIP] Start entity '{question_entity}' not in knowledge graph")
            continue
        
        try:
            # ç”Ÿæˆè·¯å¾„ï¼ˆç¡®å®šæ€§æ¨¡å¼ï¼‰
            print("\\nGenerating paths (deterministic mode):")
            deterministic_paths = generator.generate_paths(
                question=question,
                start_entity=question_entity,
                target_entities=answer_entities,
                max_paths=3,
                stochastic=False
            )
            
            if deterministic_paths:
                for j, (path, score, _) in enumerate(deterministic_paths, 1):
                    path_str = ' -> '.join(path)
                    final_entity = path[-1]
                    is_correct = final_entity in answer_entities
                    status = "âœ“" if is_correct else "âœ—"
                    print(f"  {j}. Score: {score:.4f} {status} | {path_str}")
            else:
                print("  No paths found")
            
            # ç”Ÿæˆè·¯å¾„ï¼ˆéšæœºæ¨¡å¼ï¼‰
            print("\\nGenerating paths (stochastic mode):")
            generator.enable_stochastic_exploration()
            stochastic_paths = generator.generate_paths(
                question=question,
                start_entity=question_entity,
                target_entities=answer_entities,
                max_paths=3,
                stochastic=True
            )
            generator.disable_stochastic_exploration()
            
            if stochastic_paths:
                for j, (path, score, _) in enumerate(stochastic_paths, 1):
                    path_str = ' -> '.join(path)
                    final_entity = path[-1]
                    is_correct = final_entity in answer_entities
                    status = "âœ“" if is_correct else "âœ—"
                    print(f"  {j}. Score: {score:.4f} {status} | {path_str}")
            else:
                print("  No paths found")
                
        except Exception as e:
            print(f"[ERROR] Path generation failed: {e}")
            continue
    
    # 6. æµ‹è¯•æ‰¹é‡å¤„ç†
    print(f"\n{'='*50}")
    print("TESTING BATCH PROCESSING")
    print("="*50)
    
    try:
        questions = [sample['question'] for sample in test_samples]
        start_entities = [sample['question_entity'] for sample in test_samples]
        target_entities_list = [set(sample['answer_entities']) for sample in test_samples]
        
        print("Running batch path generation...")
        batch_results = generator.forward(
            questions=questions,
            start_entities=start_entities,
            target_entities_list=target_entities_list,
            max_paths=2
        )
        
        print(f"[OK] Batch processing successful")
        print(f"Generated paths for {len(batch_results)} questions")
        
        for i, paths in enumerate(batch_results):
            print(f"  Sample {i+1}: {len(paths)} paths generated")
            
    except Exception as e:
        print(f"[ERROR] Batch processing failed: {e}")
    
    # 7. æµ‹è¯•è¯„åˆ†å‡½æ•°
    print(f"\n{'='*50}")
    print("TESTING SCORING FUNCTIONS")
    print("="*50)
    
    try:
        sample = test_samples[0]
        question = sample['question']
        start_entity = sample['question_entity']
        
        if start_entity in knowledge_graph:
            neighbors = list(knowledge_graph.neighbors(start_entity))[:3]
            print(f"Testing edge scoring for entity: {start_entity}")
            print(f"Neighbors: {neighbors}")
            
            for neighbor in neighbors:
                edge_data = knowledge_graph.get_edge_data(start_entity, neighbor)
                relation = edge_data.get('relation', 'related_to') if edge_data else 'related_to'
                
                score = generator.compute_edge_score(
                    question=question,
                    current_path=[start_entity],
                    candidate_relation=relation,
                    candidate_entity=neighbor,
                    stochastic=False
                )
                print(f"  {start_entity} -[{relation}]-> {neighbor}: {score:.4f}")
        
        print("[OK] Edge scoring functions working")
        
    except Exception as e:
        print(f"[ERROR] Edge scoring test failed: {e}")
    
    print(f"\n{'='*50}")
    print("A* GENERATOR TEST COMPLETED!")
    print("="*50)
    
    print("\\nKey Features Verified:")
    print("1. [+] A* search algorithm implementation")
    print("2. [+] Four scoring functions (s_rel, s_tran, s_loc, s_tri)")
    print("3. [+] Learnable edge score function")
    print("4. [+] Multi-hop path generation")
    print("5. [+] Stochastic vs deterministic modes")
    print("6. [+] Batch processing support")
    print("7. [+] Knowledge graph integration")
    print("8. [+] Entity embedding support")
    print("9. [+] Reserved GAN-RL training interfaces")

def load_simple_samples():
    """åŠ è½½é€‰æ‹©çš„10ä¸ªç®€å•1hopæ ·æœ¬"""
    samples_file = "selected_1hop_samples.json"
    if not Path(samples_file).exists():
        print(f"Error: {samples_file} not found. Run select_simple_1hop_samples.py first.")
        return []
    
    samples = []
    with open(samples_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                samples.append(json.loads(line))
    
    print(f"Loaded {len(samples)} simple 1hop samples")
    return samples

def load_trained_discriminator(checkpoint_path, device='cpu'):
    """åŠ è½½è®­ç»ƒå¥½çš„åˆ¤åˆ«å™¨"""
    if not Path(checkpoint_path).exists():
        print(f"Warning: Discriminator checkpoint not found at {checkpoint_path}")
        print("Creating new discriminator...")
        discriminator = EnhancedPathRankerDiscriminator(
            sbert_model_name='all-MiniLM-L6-v2',
            hidden_dim=384,  # åŒ¹é…SBERTç»´åº¦
            use_pattern_memory=False,  # ç¦ç”¨
            freeze_sbert=True
        )
        discriminator.to(device)
        return discriminator
    
    print(f"Loading trained discriminator from {checkpoint_path}")
    try:
        # å…ˆåˆ›å»ºæ¨¡å‹
        discriminator = EnhancedPathRankerDiscriminator(
            sbert_model_name='all-MiniLM-L6-v2',
            hidden_dim=384,  # åŒ¹é…SBERTç»´åº¦
            use_pattern_memory=False,  # ç¦ç”¨
            freeze_sbert=True
        )
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint = torch.load(checkpoint_path, map_location=device)
        discriminator.load_state_dict(checkpoint['model_state_dict'])
        discriminator.to(device)
        
        print("Discriminator loaded successfully")
        return discriminator
        
    except Exception as e:
        print(f"Error loading discriminator: {e}")
        print("Creating new discriminator...")
        discriminator = EnhancedPathRankerDiscriminator(
            sbert_model_name='all-MiniLM-L6-v2',
            hidden_dim=384,  # åŒ¹é…SBERTç»´åº¦
            use_pattern_memory=False,  # ç¦ç”¨
            freeze_sbert=True
        )
        discriminator.to(device)
        return discriminator

def test_adversarial_training():
    """è¿è¡Œå¯¹æŠ—å­¦ä¹ è®­ç»ƒ"""
    print("\n" + "="*80)
    print("ADVERSARIAL LEARNING TRAINING")
    print("="*80)
    
    # è®¾å¤‡é€‰æ‹© - ä¸´æ—¶ä½¿ç”¨CPUé¿å…è®¾å¤‡åŒæ­¥é—®é¢˜
    device = 'cpu'  # å¼ºåˆ¶ä½¿ç”¨CPUè¿›è¡Œæµ‹è¯•
    print(f"Using device: {device}")
    
    # åŠ è½½ç®€å•æ ·æœ¬
    print("\n1. Loading simple 1hop samples...")
    samples = load_simple_samples()
    if not samples:
        return None
    
    # æ£€æŸ¥å¿…éœ€æ–‡ä»¶
    required_files = {
        "embeddings/entity_embeddings.pt": "Entity embeddings",
        "graph/knowledge_graph.pkl": "Knowledge graph"
    }
    
    for file_path, description in required_files.items():
        if not Path(file_path).exists():
            print(f"Error: {description} not found at {file_path}")
            return None
    
    # åˆå§‹åŒ–ç”Ÿæˆå™¨
    print("\n2. Initializing Generator...")
    try:
        # ä½¿ç”¨ä¿®å¤åçš„å¯å¾®åˆ†ç”Ÿæˆå™¨
        generator = DifferentiablePathGeneratorTrulyFixed(
            entity_embedding_path="embeddings/entity_embeddings.pt",
            max_path_length=6,
            beam_width=5
        )
        generator.to(device)
        
        # åŠ è½½çŸ¥è¯†å›¾è°±
        with open("graph/knowledge_graph.pkl", 'rb') as f:
            knowledge_graph = pickle.load(f)
        
        # åˆ›å»ºå…³ç³»è¯è¡¨
        relations = set()
        for _, _, edge_data in knowledge_graph.edges(data=True):
            relation = edge_data.get('relation', 'related_to')
            relations.add(relation)
        relation_to_id = {rel: idx for idx, rel in enumerate(relations)}
        
        generator.set_knowledge_graph(knowledge_graph, relation_to_id)
        print(f"Generator initialized: {len(knowledge_graph.nodes)} nodes, {len(knowledge_graph.edges)} edges")
        
    except Exception as e:
        print(f"Error initializing generator: {e}")
        return None
    
    # åŠ è½½åˆ¤åˆ«å™¨
    print("\n3. Loading Discriminator...")
    discriminator_path = "checkpoints/enhanced_path_ranker_epoch_10.pt"  # ä½¿ç”¨æœ€æ–°çš„æ£€æŸ¥ç‚¹
    discriminator = load_trained_discriminator(discriminator_path, device)
    
    # åˆå§‹åŒ–GAN-RLè®­ç»ƒå™¨
    print("\n4. Initializing GAN-RL Trainer...")
    try:
        trainer = GANRLTrainer(
            generator=generator,
            discriminator=discriminator,
            device=device
        )
        print("GAN-RL Trainer initialized successfully")
    except Exception as e:
        print(f"Error initializing trainer: {e}")
        return None
    
    # å¼€å§‹å¯¹æŠ—è®­ç»ƒ
    print("\n5. Starting Adversarial Training...")
    training_results = {
        'start_time': datetime.now().isoformat(),
        'device': device,
        'samples_used': len(samples),
        'epochs': [],
        'final_stats': {},
        'sample_details': samples
    }
    
    num_epochs = 3  # å°è§„æ¨¡æµ‹è¯•
    
    for epoch in range(num_epochs):
        print(f"\n--- Epoch {epoch + 1}/{num_epochs} ---")
        epoch_start_time = time.time()
        
        try:
            # è¿è¡Œä¸€ä¸ªè®­ç»ƒepoch
            epoch_stats = trainer.train_epoch(
                qa_dataset=samples,
                discriminator_steps=2,  # è¾ƒå°‘çš„æ­¥æ•°ç”¨äºå¿«é€Ÿæµ‹è¯•
                generator_steps=1
            )
            
            epoch_time = time.time() - epoch_start_time
            
            # è®°å½•epochç»Ÿè®¡
            epoch_result = {
                'epoch': epoch + 1,
                'discriminator_loss': float(epoch_stats.get('discriminator_loss', 0.0)),
                'generator_reward': float(epoch_stats.get('generator_reward', 0.0)),
                'epoch_time': epoch_time,
                'timestamp': datetime.now().isoformat()
            }
            
            training_results['epochs'].append(epoch_result)
            
            print(f"Epoch {epoch + 1} completed in {epoch_time:.2f}s")
            print(f"  Discriminator Loss: {epoch_result['discriminator_loss']:.4f}")
            print(f"  Generator Reward: {epoch_result['generator_reward']:.4f}")
            
        except Exception as e:
            print(f"Error in epoch {epoch + 1}: {e}")
            epoch_result = {
                'epoch': epoch + 1,
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
            training_results['epochs'].append(epoch_result)
            continue
    
    # ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
    print("\n6. Saving Trained Models...")
    try:
        import os
        
        # 1. ä¿å­˜å®Œæ•´è®­ç»ƒçŠ¶æ€ï¼ˆç”¨äºç»§ç»­è®­ç»ƒï¼‰
        checkpoint_dir = "checkpoints"
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        checkpoint_path = f"{checkpoint_dir}/gan_rl_adversarial_training_final.pt"
        trainer.save_checkpoint(checkpoint_path, epochs)
        
        print(f"âœ… Complete training state saved to: {checkpoint_path}")
        
        # 2. ä¿å­˜ç”Ÿäº§ç¯å¢ƒæ¨¡å‹ï¼ˆç”¨äºæ¨ç†ï¼‰
        from advanced_inference_system import save_production_models
        production_dir = save_production_models(trainer, "checkpoints/production_models")
        
        print(f"âœ… Production models saved to: {production_dir}")
        print("ğŸ“ Production structure:")
        print("  â”œâ”€â”€ discoverer_model.pt      # ç”Ÿæˆå™¨æƒé‡")
        print("  â”œâ”€â”€ ranker_model.pt          # åˆ¤åˆ«å™¨æƒé‡")  
        print("  â”œâ”€â”€ model_metadata.json      # æ¨¡å‹å…ƒä¿¡æ¯")
        print("  â””â”€â”€ training_history.json    # è®­ç»ƒå†å²")
        
    except Exception as e:
        print(f"âŒ Error saving models: {e}")
    
    # æœ€ç»ˆè¯„ä¼° - ç”Ÿæˆå™¨ä¸åˆ¤åˆ«å™¨è¯„åˆ†åˆ†æ
    print("\n7. Generator-Discriminator Scoring Analysis...")
    try:
        evaluation_results = []
        
        for i, sample in enumerate(samples):  # æµ‹è¯•æ‰€æœ‰æ ·æœ¬
            question = sample['question']
            question_entity = sample['question_entity']
            answer_entities = set(sample['answer_entities'])
            
            print(f"\n=== Sample {i+1} ===")
            print(f"Start: {question_entity}")
            
            # ç”Ÿæˆè·¯å¾„
            try:
                generated_paths = generator.generate_paths(
                    question=question,
                    start_entity=question_entity,
                    target_entities=answer_entities,
                    max_paths=5,
                    stochastic=False
                )
                
                if not generated_paths:
                    print("  No paths generated")
                    continue
                
                # åˆ†æç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨è¯„åˆ†
                path_analysis = []
                for j, (path, gen_score, _) in enumerate(generated_paths):
                    if len(path) == 0:
                        continue
                        
                    final_entity = path[-1]
                    path_string = '.'.join(path)
                    path_data = [{'paths': {final_entity: [path_string]}}]
                    
                    # è·å–åˆ¤åˆ«å™¨åˆ†æ•°
                    with torch.no_grad():
                        disc_output = discriminator([question], path_data, epoch=0)
                        disc_raw = float(disc_output[0]['individual_scores'][0])
                        disc_score = float(torch.sigmoid(disc_output[0]['individual_scores'][0]))
                    
                    is_correct = final_entity in answer_entities
                    
                    print(f"  Path {j+1}: {question_entity} -> {final_entity}")
                    print(f"    Generator Score: {gen_score:.4f}")
                    print(f"    Discriminator Raw: {disc_raw:.4f}")
                    print(f"    Discriminator Sigmoid: {disc_score:.4f}")
                    print(f"    Target Hit: {'YES' if is_correct else 'NO'}")
                    
                    path_analysis.append({
                        'path_index': j + 1,
                        'start_entity': question_entity,
                        'end_entity': final_entity,
                        'generator_score': float(gen_score),
                        'discriminator_raw': disc_raw,
                        'discriminator_sigmoid': disc_score,
                        'hits_target': is_correct,
                        'path_length': len(path)
                    })
                
                # åˆ†æç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨ä¸€è‡´æ€§
                if len(path_analysis) >= 2:
                    best_gen = max(path_analysis, key=lambda x: x['generator_score'])
                    best_disc = max(path_analysis, key=lambda x: x['discriminator_sigmoid'])
                    
                    print(f"  Generator Best: Path {best_gen['path_index']} (Score: {best_gen['generator_score']:.4f})")
                    print(f"  Discriminator Best: Path {best_disc['path_index']} (Score: {best_disc['discriminator_sigmoid']:.4f})")
                    
                    if best_gen['path_index'] == best_disc['path_index']:
                        print("  Agreement: CONSISTENT")
                    else:
                        print("  Agreement: DISAGREEMENT")
                
                evaluation_results.append({
                    'sample_index': i + 1,
                    'start_entity': question_entity,
                    'path_analysis': path_analysis,
                    'num_paths_generated': len(path_analysis)
                })
                
            except Exception as e:
                print(f"  Error: {e}")
                evaluation_results.append({
                    'sample_index': i + 1,
                    'start_entity': question_entity,
                    'error': str(e)
                })
        
        training_results['scoring_analysis'] = evaluation_results
        
    except Exception as e:
        print(f"Error in scoring analysis: {e}")
        training_results['scoring_analysis_error'] = str(e)
    
    # æ·»åŠ æœ€ç»ˆç»Ÿè®¡
    training_results['end_time'] = datetime.now().isoformat()
    training_results['total_training_time'] = sum(
        epoch.get('epoch_time', 0) for epoch in training_results['epochs']
    )
    training_results['final_stats'] = {
        'generator_stats': trainer.training_stats.get('generator_rewards', []),
        'discriminator_stats': trainer.training_stats.get('discriminator_losses', []),
    }
    
    # ä¿å­˜ç»“æœ
    print("\n8. Saving Results...")
    output_file = f"adversarial_training_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(training_results, f, indent=2, ensure_ascii=False)
    
    print(f"Results saved to {output_file}")
    print(f"Total training time: {training_results['total_training_time']:.2f}s")
    
    return training_results

if __name__ == "__main__":
    print("Choose test mode:")
    print("1. Basic A* Generator Test")
    print("2. Adversarial Learning Training")
    print("3. Both")
    
    choice = "2"  # ç›´æ¥è¿è¡Œå¯¹æŠ—å­¦ä¹ è®­ç»ƒæµ‹è¯•
    
    if choice in ['1', '3']:
        test_astar_generator()
    
    if choice in ['2', '3']:
        test_adversarial_training()