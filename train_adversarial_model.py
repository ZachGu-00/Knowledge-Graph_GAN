"""
å¯¹æŠ—å­¦ä¹ çŸ¥è¯†å›¾è°±è·¯å¾„æŽ¨ç† - æ­£å¼è®­ç»ƒè„šæœ¬

ä½¿ç”¨ä¿®å¤ç‰ˆGAN-RLè®­ç»ƒå™¨ï¼ŒåŒ…å«å®Œæ•´æŒ‡æ ‡ç›‘æŽ§å’ŒJSONæ—¥å¿—è®°å½•

è¿è¡Œå‘½ä»¤ï¼š
python train_adversarial_model.py

è®­ç»ƒæŒ‡æ ‡ï¼š
ä¸€ã€åˆ¤åˆ«å™¨ (Ranker) æŒ‡æ ‡ï¼šLoss_D, Acc_Real, Acc_Fake, F1-Score
äºŒã€ç”Ÿæˆå™¨ (Discoverer) æŒ‡æ ‡ï¼šLoss_G, Avg_Reward, Path_Length_Avg, Path_Diversity 
ä¸‰ã€ç«¯åˆ°ç«¯ä»»åŠ¡æŒ‡æ ‡ï¼šHits@1, MRR, Success_Rate
"""

import sys
import torch
import pickle
import json
import random
import time
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Set

# æ·»åŠ modelsè·¯å¾„
sys.path.append(str(Path(__file__).parent / "models" / "path_discover"))
sys.path.append(str(Path(__file__).parent / "models" / "path_ranker"))

from models.path_discover.beam_search_generator import BeamSearchPathGenerator
from enhanced_path_ranker import EnhancedPathRankerDiscriminator
from gan_rl_trainer_fixed import GANRLTrainerFixed

class AdversarialTrainingPipeline:
    """å¯¹æŠ—å­¦ä¹ è®­ç»ƒæµæ°´çº¿"""
    
    def __init__(self):
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"Using device: {self.device}")
        
        self.generator = None
        self.discriminator = None
        self.trainer = None
        
        # æ•°æ®
        self.knowledge_graph = None
        self.train_data = {}  # æŒ‰hopç±»åž‹ç»„ç»‡çš„è®­ç»ƒæ•°æ®
        
        # è®­ç»ƒæ—¥å¿—
        self.training_log = {
            'training_config': {},
            'epoch_metrics': [],
            'final_summary': {}
        }
        
        # æœ€ä½³æ¨¡åž‹è·Ÿè¸ª
        self.best_metrics = {
            'best_hits_at_1': 0.0,
            'best_mrr': 0.0,
            'best_success_rate': 0.0,
            'best_epoch': 0
        }
    
    def load_dataset(self):
        """åŠ è½½æ•°æ®é›†"""
        print("Loading dataset...")
        
        # 1. åŠ è½½QAæ•°æ®
        qa_file = "query/qa_with_paths_cleaned.json"
        if not Path(qa_file).exists():
            print(f"ERROR: QA dataset not found: {qa_file}")
            return False
        
        all_data = self._load_multiline_json(qa_file)
        if not all_data:
            return False
        
        # 2. æŒ‰hopåˆ†ç»„è®­ç»ƒæ•°æ®
        self.train_data = {'1hop': [], '2hop': [], '3hop': []}
        
        for item in all_data:
            type_field = item.get('type', '')
            if 'train' in type_field:
                if '1hop' in type_field:
                    self.train_data['1hop'].append(item)
                elif '2hop' in type_field:
                    self.train_data['2hop'].append(item)
                elif '3hop' in type_field:
                    self.train_data['3hop'].append(item)
        
        total_train = sum(len(data) for data in self.train_data.values())
        print(f"Training data loaded:")
        for hop_type, data in self.train_data.items():
            print(f"  {hop_type}: {len(data)} samples")
        print(f"  Total: {total_train} samples")
        
        # 3. åŠ è½½çŸ¥è¯†å›¾è°±
        kg_file = "graph/knowledge_graph.pkl"
        if not Path(kg_file).exists():
            print(f"ERROR: Knowledge graph not found: {kg_file}")
            return False
        
        with open(kg_file, 'rb') as f:
            self.knowledge_graph = pickle.load(f)
        
        print(f"Knowledge graph: {self.knowledge_graph.number_of_nodes()} nodes, {self.knowledge_graph.number_of_edges()} edges")
        
        return True
    
    def _load_multiline_json(self, file_path: str) -> List[Dict]:
        """åŠ è½½å¤šè¡ŒJSONæ ¼å¼çš„æ–‡ä»¶"""
        data = []
        current_obj = ""
        brace_count = 0
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    
                    current_obj += line + "\n"
                    brace_count += line.count('{') - line.count('}')
                    
                    if brace_count == 0 and current_obj.strip():
                        try:
                            obj = json.loads(current_obj.strip())
                            data.append(obj)
                            current_obj = ""
                        except json.JSONDecodeError:
                            current_obj = ""
        except Exception as e:
            print(f"Error loading {file_path}: {e}")
            return []
        
        return data
    
    def initialize_models(self):
        """åˆå§‹åŒ–æ¨¡åž‹"""
        print("Initializing models...")
        
        if not Path("embeddings/entity_embeddings.pt").exists():
            print("ERROR: Entity embeddings not found. Run: python create_embeddings.py")
            return False
        
        try:
            # ç”Ÿæˆå™¨ - æ–°çš„Beam Searchç”Ÿæˆå™¨
            self.generator = BeamSearchPathGenerator(
                entity_embedding_path="embeddings/entity_embeddings.pt",
                max_path_length=6,
                beam_width=5
            )
            self.generator.load_knowledge_graph("graph/knowledge_graph.pkl")
            
            # åˆ¤åˆ«å™¨
            self.discriminator = EnhancedPathRankerDiscriminator(
                freeze_sbert=True,
                use_pattern_memory=False
            )
            
            # åŠ è½½é¢„è®­ç»ƒæƒé‡
            try:
                checkpoint_path = "checkpoints/enhanced_pathranker/best_hits1_model.pth"
                print(f"Loading discriminator weights from: {checkpoint_path}")
                checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)
                
                # æå–model_state_dict
                if 'model_state_dict' in checkpoint:
                    model_weights = checkpoint['model_state_dict']
                else:
                    model_weights = checkpoint
                
                self.discriminator.load_state_dict(model_weights, strict=False)
                print("Pre-trained discriminator weights loaded successfully")
            except Exception as e:
                print(f"Warning: Failed to load discriminator weights: {e}")
                print("Using randomly initialized discriminator")
            
            # è®­ç»ƒå™¨ - è®¾ç½®åˆ¤åˆ«å™¨é˜ˆå€¼ä¸º0.8
            self.trainer = GANRLTrainerFixed(
                self.generator, 
                self.discriminator, 
                device=self.device,
                discriminator_threshold=0.8  # è®¾ç½®é˜ˆå€¼ä¸º0.8
            )
            
            self.generator.to(self.device)
            self.discriminator.to(self.device)
            
            print("Models initialized successfully")
            return True
            
        except Exception as e:
            print(f"Model initialization failed: {e}")
            return False
    
    def train_with_metrics(self, epochs: int = 15):
        """æ‰§è¡Œå¸¦å®Œæ•´æŒ‡æ ‡ç›‘æŽ§çš„å¯¹æŠ—è®­ç»ƒ"""
        print(f"Starting adversarial training ({epochs} epochs)")
        print("="*80)
        
        # è®­ç»ƒé…ç½®
        self.training_log['training_config'] = {
            'epochs': epochs,
            'device': str(self.device),
            'start_time': datetime.now().isoformat(),
            'data_distribution': {hop: len(data) for hop, data in self.train_data.items()},
            'model_types': {
                'generator': type(self.generator).__name__,
                'discriminator': type(self.discriminator).__name__,
                'trainer': type(self.trainer).__name__
            }
        }
        
        for epoch in range(epochs):
            epoch_start = time.time()
            print(f"\nEPOCH {epoch + 1}/{epochs}")
            print("-" * 60)
            
            try:
                # éšæœºé€‰æ‹©hopç±»åž‹è®­ç»ƒæ•°æ®
                epoch_plan = self._create_epoch_plan(epoch)
                if not epoch_plan:
                    continue
                
                # åˆå¹¶æœ¬epochçš„è®­ç»ƒæ•°æ®
                epoch_data = []
                for hop_type, samples in epoch_plan.items():
                    epoch_data.extend(samples)
                
                # æ‰§è¡Œå¯¹æŠ—è®­ç»ƒï¼ˆå¸¦å®Œæ•´æŒ‡æ ‡è®¡ç®—ï¼‰
                epoch_metrics = self.trainer.train_epoch_with_adversarial_correction(
                    epoch_data, current_epoch=epoch
                )
                
                # æ·»åŠ æ—¶é—´å’Œepochä¿¡æ¯
                epoch_metrics.update({
                    'epoch': epoch + 1,
                    'training_time': time.time() - epoch_start,
                    'timestamp': datetime.now().isoformat(),
                    'hop_distribution': {hop: len(samples) for hop, samples in epoch_plan.items()},
                    'total_samples': len(epoch_data)
                })
                
                # ä¿å­˜epochæŒ‡æ ‡åˆ°æ—¥å¿—
                self.training_log['epoch_metrics'].append(epoch_metrics)
                
                # æ¯ä¸ªepochä¿å­˜æ—¥å¿—
                self._save_training_log()
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿å­˜æœ€ä½³æ¨¡åž‹
                self._check_and_save_best_model(epoch_metrics, epoch)
                
            except Exception as e:
                print(f"Epoch {epoch + 1} failed: {e}")
                import traceback
                traceback.print_exc()
        
        # å®Œæˆè®­ç»ƒ
        self._finalize_training()
    
    def _create_epoch_plan(self, epoch: int) -> Dict[str, List[Dict]]:
        """åˆ›å»ºepochè®­ç»ƒè®¡åˆ’"""
        hop_types = ['1hop', '2hop', '3hop']
        random.shuffle(hop_types)
        
        epoch_plan = {}
        for hop_type in hop_types:
            available_data = self.train_data[hop_type]
            if not available_data:
                continue
            
            # åŠ¨æ€é‡‡æ · - æŒ‰hopç±»åž‹åˆ†é…ä¸åŒæ•°é‡
            if hop_type == '1hop' and len(available_data) > 1000:
                sample_count = 1000  # 1hop: 1000ä¸ª
                selected_samples = random.sample(available_data, sample_count)
            elif hop_type == '2hop' and len(available_data) > 2000:
                sample_count = 2000  # 2hop: 2000ä¸ª  
                selected_samples = random.sample(available_data, sample_count)
            elif hop_type == '3hop' and len(available_data) > 3000:
                sample_count = 3000  # 3hop: 3000ä¸ª
                selected_samples = random.sample(available_data, sample_count)
            else:
                selected_samples = available_data.copy()
                random.shuffle(selected_samples)
            
            epoch_plan[hop_type] = selected_samples
        
        return epoch_plan
    
    def _check_and_save_best_model(self, epoch_metrics, epoch):
        """æ£€æŸ¥å¹¶ä¿å­˜æœ€ä½³æ¨¡åž‹"""
        current_hits_at_1 = epoch_metrics.get('Hits_at_1', 0.0)
        current_mrr = epoch_metrics.get('MRR', 0.0)
        current_success_rate = epoch_metrics.get('Success_Rate', 0.0)
        
        # ç»¼åˆæŒ‡æ ‡ï¼šåŠ æƒå¹³å‡
        current_score = (current_hits_at_1 * 0.4 + current_mrr * 0.4 + current_success_rate * 0.2)
        best_score = (self.best_metrics['best_hits_at_1'] * 0.4 + 
                     self.best_metrics['best_mrr'] * 0.4 + 
                     self.best_metrics['best_success_rate'] * 0.2)
        
        if current_score > best_score:
            print(f"\nðŸ† NEW BEST MODEL! Score: {current_score:.4f} (prev: {best_score:.4f})")
            
            # æ›´æ–°æœ€ä½³æŒ‡æ ‡
            self.best_metrics.update({
                'best_hits_at_1': current_hits_at_1,
                'best_mrr': current_mrr,
                'best_success_rate': current_success_rate,
                'best_epoch': epoch + 1,
                'best_score': current_score
            })
            
            # ä¿å­˜æœ€ä½³æ¨¡åž‹åˆ°adversarialç›®å½•
            try:
                from advanced_inference_system import save_production_models
                best_model_dir = save_production_models(
                    self.trainer, 
                    f"checkpoints/adversarial/best_epoch_{epoch+1}"
                )
                print(f"ðŸ’Ž Best model saved: {best_model_dir}")
                
                # ä¹Ÿæ›´æ–°ä¸»adversarialç›®å½•
                production_dir = save_production_models(
                    self.trainer, 
                    "checkpoints/adversarial"
                )
                print(f"ðŸ”„ Adversarial models updated: {production_dir}")
                
            except Exception as e:
                print(f"âŒ Failed to save best model: {e}")
    
    def _save_training_log(self):
        """ä¿å­˜è®­ç»ƒæ—¥å¿—"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = f"training_log_{timestamp}.json"
        
        try:
            with open(log_file, 'w', encoding='utf-8') as f:
                json.dump(self.training_log, f, indent=2, ensure_ascii=False)
            print(f"Training log saved: {log_file}")
        except Exception as e:
            print(f"Failed to save training log: {e}")
    
    def _finalize_training(self):
        """å®Œæˆè®­ç»ƒå¹¶ä¿å­˜æœ€ç»ˆæ¨¡åž‹"""
        print("\nFinalizing training...")
        
        # æœ€ç»ˆæ€»ç»“
        if self.training_log['epoch_metrics']:
            final_metrics = self.training_log['epoch_metrics'][-1]
            
            # è®¡ç®—è¶‹åŠ¿
            first_metrics = self.training_log['epoch_metrics'][0]
            improvements = {
                'Loss_D_change': final_metrics.get('Loss_D', 0) - first_metrics.get('Loss_D', 0),
                'Avg_Reward_change': final_metrics.get('Avg_Reward', 0) - first_metrics.get('Avg_Reward', 0),
                'Hits_at_1_change': final_metrics.get('Hits_at_1', 0) - first_metrics.get('Hits_at_1', 0),
                'MRR_change': final_metrics.get('MRR', 0) - first_metrics.get('MRR', 0)
            }
            
            self.training_log['final_summary'] = {
                'completion_time': datetime.now().isoformat(),
                'total_epochs': len(self.training_log['epoch_metrics']),
                'final_metrics': final_metrics,
                'improvements': improvements
            }
        
        # ä¿å­˜æœ€ç»ˆæ—¥å¿—
        self._save_training_log()
        
        # ä¿å­˜æœ€ç»ˆå¯¹æŠ—æ¨¡åž‹
        try:
            from advanced_inference_system import save_production_models
            production_dir = save_production_models(
                self.trainer, 
                "checkpoints/adversarial"
            )
            print(f"Final adversarial models saved: {production_dir}")
        except Exception as e:
            print(f"Failed to save adversarial models: {e}")
        
        # æ˜¾ç¤ºæœ€ç»ˆæŠ¥å‘Š
        self._display_final_report()
    
    def _display_final_report(self):
        """æ˜¾ç¤ºæœ€ç»ˆè®­ç»ƒæŠ¥å‘Š"""
        print("\n" + "="*80)
        print("ADVERSARIAL TRAINING COMPLETED!")
        print("="*80)
        
        if not self.training_log['epoch_metrics']:
            print("No training metrics available.")
            return
        
        final_metrics = self.training_log['epoch_metrics'][-1]
        improvements = self.training_log['final_summary'].get('improvements', {})
        
        print("\nFINAL PERFORMANCE METRICS:")
        print(f"  Loss_D:           {final_metrics.get('Loss_D', 0):.4f}")
        print(f"  Acc_Real:         {final_metrics.get('Acc_Real', 0):.3f}")
        print(f"  Acc_Fake:         {final_metrics.get('Acc_Fake', 0):.3f}")
        print(f"  F1-Score:         {final_metrics.get('F1_Score', 0):.3f}")
        print(f"  Avg_Reward:       {final_metrics.get('Avg_Reward', 0):.4f}")
        print(f"  Path_Length_Avg:  {final_metrics.get('Path_Length_Avg', 0):.2f}")
        print(f"  Path_Diversity:   {final_metrics.get('Path_Diversity', 0):.3f}")
        print(f"  Hits@1:           {final_metrics.get('Hits_at_1', 0):.3f}")
        print(f"  MRR:              {final_metrics.get('MRR', 0):.3f}")
        print(f"  Success_Rate:     {final_metrics.get('Success_Rate', 0):.3f}")
        
        print("\nTRAINING IMPROVEMENTS:")
        for metric, change in improvements.items():
            direction = "â†‘" if change > 0 else "â†“" if change < 0 else "â†’"
            print(f"  {metric}: {change:+.4f} {direction}")
        
        print("\nNEXT STEPS:")
        print("  1. Test inference: python demo_advanced_inference.py")
        print("  2. View training logs in current directory")
        print("  3. Deploy models from checkpoints/production_models/")
        print("="*80)

def main():
    """ä¸»è®­ç»ƒç¨‹åº"""
    print("ADVERSARIAL KNOWLEDGE GRAPH PATH REASONING")
    print("Training Pipeline with Comprehensive Metrics")
    print("="*80)
    
    # åˆ›å»ºè®­ç»ƒæµæ°´çº¿
    pipeline = AdversarialTrainingPipeline()
    
    # åŠ è½½æ•°æ®
    if not pipeline.load_dataset():
        print("ERROR: Dataset loading failed")
        return
    
    # åˆå§‹åŒ–æ¨¡åž‹
    if not pipeline.initialize_models():
        print("ERROR: Model initialization failed")
        return
    
    # å¼€å§‹è®­ç»ƒ
    print("\nStarting adversarial training with:")
    print("* Fixed GAN-RL trainer (handles discriminator fooling)")
    print("* Ground Truth correction mechanism")
    print("* Intelligent reward shaping")
    print("* Comprehensive metrics monitoring")
    print("* JSON logging for all epochs")
    
    pipeline.train_with_metrics(epochs=5)

if __name__ == "__main__":
    main()